# News URL Scraper para Sites Governamentais Brasileiros

Uma ferramenta robusta e modular para extrair URLs de notÃ­cias de sites do governo brasileiro e gerenciar configuraÃ§Ãµes de URLs de forma inteligente.

## ğŸš€ Funcionalidades Principais

### **1. Raspagem Inteligente de URLs (`scrape`)**
- **Entrada**: Lista de sites governamentais em CSV
- **SaÃ­da**: URLs de notÃ­cias extraÃ­das com alta precisÃ£o
- **Taxa de Sucesso**: 99.4% (161/162 sites processados)

### **2. Gerenciamento de ConfiguraÃ§Ã£o (`update_urls`)**
- **Entrada**: URLs extraÃ­das + configuraÃ§Ã£o YAML existente
- **SaÃ­da**: ConfiguraÃ§Ã£o YAML atualizada com validaÃ§Ã£o inteligente
- **Taxa de PrecisÃ£o**: 88.6% de concordÃ¢ncia com fonte da verdade

## ğŸ“ Arquitetura Modular

```
sites-url-finder/
â”œâ”€â”€ main.py                    # ğŸ¯ Interface CLI principal
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py           # Pacote Python
â”‚   â”œâ”€â”€ scraper.py            # ğŸ” Motor de extraÃ§Ã£o de URLs
â”‚   â””â”€â”€ url_updater.py        # ğŸ”„ Gerenciador de configuraÃ§Ã£o YAML
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/                # ğŸ“¥ Arquivos de entrada
â”‚   â”‚   â”œâ”€â”€ all-govbr-sites.csv
â”‚   â”‚   â””â”€â”€ site_urls.yaml
â”‚   â”œâ”€â”€ stage/                # ğŸ”„ Resultados intermediÃ¡rios
â”‚   â”‚   â””â”€â”€ scraped_urls.csv
â”‚   â””â”€â”€ output/               # ğŸ“¤ Resultados finais
â”‚       â””â”€â”€ site_urls.yaml
â”œâ”€â”€ tests/                    # ğŸ§ª Testes unitÃ¡rios (24 testes)
â””â”€â”€ requirements.txt          # DependÃªncias
```

## ğŸ› ï¸ InstalaÃ§Ã£o

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/nitaibezerra/govbr-news-url.git
cd govbr-news-url

# 2. Instale as dependÃªncias
pip install -r requirements.txt

# 3. Verifique a instalaÃ§Ã£o
python main.py --help
```

## ğŸ“– Como Usar

### **Comando 1: Extrair URLs de NotÃ­cias**

```bash
python main.py scrape
```

**O que faz:**
- LÃª sites de `data/input/all-govbr-sites.csv`
- Extrai URLs de notÃ­cias usando 4 estratÃ©gias inteligentes:
  1. **RodapÃ©**: "NotÃ­cias" em `<div class="footer-wrapper">`
  2. **Fallback 1**: "Ãšltimas NotÃ­cias" em toda a pÃ¡gina
  3. **Fallback 2**: "Mais NotÃ­cias" em toda a pÃ¡gina  
  4. **Fallback 3**: "NotÃ­cias" genÃ©rico (com filtros anti-promocionais)
- Salva resultados em `data/stage/scraped_urls.csv`

**Exemplo de saÃ­da:**
```
ğŸš€ Iniciando raspagem de URLs de notÃ­cias...
ğŸ“‚ Entrada: data/input/all-govbr-sites.csv
ğŸ“‚ SaÃ­da: data/stage/scraped_urls.csv

âœ… Raspagem concluÃ­da!
ğŸ“Š 161/162 sites com links de notÃ­cias
ğŸ“ˆ Taxa de sucesso: 99.4%
ğŸ’¾ Resultados salvos em: data/stage/scraped_urls.csv
```

### **Comando 2: Atualizar ConfiguraÃ§Ã£o YAML**

```bash
python main.py update_urls
```

**O que faz:**
- Compara URLs extraÃ­das com configuraÃ§Ã£o existente (`data/input/site_urls.yaml`)
- **URLs Contidas**: Considera vÃ¡lidas URLs extraÃ­das que estÃ£o contidas nas corretas
- **Novas AgÃªncias**: Adiciona automaticamente agÃªncias descobertas
- **RelatÃ³rio**: Mostra discrepÃ¢ncias encontradas
- Gera configuraÃ§Ã£o atualizada em `data/output/site_urls.yaml`

**Exemplo de saÃ­da:**
```
ğŸ”„ Iniciando atualizaÃ§Ã£o de URLs...
ğŸ“‚ CSV de entrada: data/stage/scraped_urls.csv
ğŸ“‚ YAML de entrada: data/input/site_urls.yaml
ğŸ“‚ YAML de saÃ­da: data/output/site_urls.yaml

ğŸ“Š RELATÃ“RIO DE DISCREPÃ‚NCIAS (9 encontradas)
============================================================

1. **AGRICULTURA**
   Portal: https://www.gov.br/agricultura/pt-br
   âŒ ExtraÃ­do: https://www.gov.br/agricultura/pt-br/campanhas/g20-agro-2024/noticias-da-g20-2
   âœ… Correto:  https://www.gov.br/agricultura/pt-br/assuntos/noticias

âœ… AtualizaÃ§Ã£o concluÃ­da!
ğŸ“Š 70/79 URLs corretos
ğŸ“ˆ Taxa de acerto: 88.6%
ğŸ’¾ YAML atualizado salvo em: data/output/site_urls.yaml
```

## ğŸ¯ Pipeline Completo

Execute o pipeline completo de extraÃ§Ã£o e atualizaÃ§Ã£o:

```bash
# Passo 1: Extrair URLs de notÃ­cias
python main.py scrape

# Passo 2: Atualizar configuraÃ§Ã£o YAML
python main.py update_urls

# Resultado: data/output/site_urls.yaml atualizado
```

## ğŸ§  Algoritmos Inteligentes

### **1. SeleÃ§Ã£o Inteligente de Links**
O sistema usa um algoritmo de pontuaÃ§Ã£o para escolher o melhor link quando mÃºltiplas opÃ§Ãµes existem:

```python
# PriorizaÃ§Ã£o automÃ¡tica:
# ğŸ† Links com 'comunicacao' no URL: +100 pontos
# ğŸ¥ˆ Texto "Ãšltimas NotÃ­cias": +50 pontos  
# ğŸ¥‰ Caminhos mais curtos: +10 pontos
# ğŸ’¡ URLs com 'noticias': +5 pontos
```

### **2. Matching Prioritizado de Texto**
Busca por links usando prioridades:

```python
# Ordem de prioridade:
# 1. ğŸ¯ CorrespondÃªncia exata: "NotÃ­cias"
# 2. ğŸ”š Termina com: "Principais NotÃ­cias" 
# 3. ğŸ”œ ComeÃ§a com: "NotÃ­cias Siscomex"
```

### **3. Filtros Anti-Promocionais**
Remove automaticamente links promocionais especÃ­ficos:

```python
# ğŸš« Filtra automaticamente:
skip_patterns = ['g20', 'evento', 'campanha', 'especial', 'promocao']
```

### **4. ValidaÃ§Ã£o de URLs Contidas**
Considera como vÃ¡lidas URLs extraÃ­das que estÃ£o contidas nas URLs corretas:

```python
# Exemplo: âœ… VÃ¡lido
extracted = "https://www.gov.br/agency/pt-br/noticias"
correct   = "https://www.gov.br/agency/pt-br/noticias/ultimas"
# extracted estÃ¡ contido em correct â†’ Aceito como vÃ¡lido
```

## ğŸ“Š MÃ©tricas de Performance

### **ExtraÃ§Ã£o de URLs**
- âœ… **Taxa de Sucesso**: 99.4% (161/162 sites)
- ğŸ”„ **EstratÃ©gias de Fallback**: 4 nÃ­veis
- âš¡ **Processamento**: ~2 segundos por site
- ğŸ’¾ **Salvamento**: A cada 5 sites processados

### **ValidaÃ§Ã£o YAML**
- âœ… **Taxa de PrecisÃ£o**: 88.6% (70/79 agÃªncias corretas)
- ğŸ†• **Descobertas**: +79 novas agÃªncias adicionadas
- ğŸ¯ **DiscrepÃ¢ncias**: Apenas 9 restantes
- ğŸ§  **URLs Contidas**: ValidaÃ§Ã£o inteligente ativa

### **Casos de Uso Reais**
- ğŸ›ï¸ **162 sites governamentais** processados
- ğŸ“° **161 URLs de notÃ­cias** extraÃ­das com sucesso
- ğŸ” **79 agÃªncias novas** descobertas e integradas
- âš™ï¸ **88.6% de precisÃ£o** na validaÃ§Ã£o com fonte da verdade

## ğŸ§ª Testes

Execute a suÃ­te completa de testes:

```bash
# Todos os testes
python -m pytest tests/ -v

# Testes especÃ­ficos
python -m pytest tests/test_scraper.py -v
python -m pytest tests/test_url_updater.py -v
python -m pytest tests/test_main.py -v

# Com cobertura
python -m pytest tests/ --cov=src --cov-report=html
```

**Status dos Testes:**
- âœ… **24/24 testes passando**
- ğŸ§ª **Cobertura completa** dos mÃ³dulos principais
- ğŸ”§ **Testes de integraÃ§Ã£o** incluÃ­dos
- ğŸŒ **Suporte a caracteres nÃ£o-ASCII** verificado

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### **Personalizar ParÃ¢metros do Scraper**

```python
from src.scraper import NewsLinkScraper

scraper = NewsLinkScraper(
    request_timeout=30,     # Timeout das requisiÃ§Ãµes (segundos)
    request_delay=2.0,      # Delay entre requisiÃ§Ãµes (segundos)  
    user_agent="Custom-Bot/1.0"  # User agent customizado
)
```

### **Configurar Logging**

```python
import logging

# Ativar logs detalhados
logging.basicConfig(level=logging.INFO)

# Logs apenas de erros
logging.basicConfig(level=logging.ERROR)
```

## ğŸš¨ SoluÃ§Ã£o de Problemas

### **Erro: Arquivo nÃ£o encontrado**
```bash
âŒ Arquivo de entrada nÃ£o encontrado: data/input/all-govbr-sites.csv
```
**SoluÃ§Ã£o**: Certifique-se de que o arquivo existe em `data/input/`

### **Erro: Import failed**
```bash
ImportError: cannot import name 'NewsLinkScraper'
```
**SoluÃ§Ã£o**: Instale as dependÃªncias: `pip install -r requirements.txt`

### **Taxa de sucesso baixa**
- âœ… Verifique conectividade de rede
- âœ… Alguns sites podem estar temporariamente indisponÃ­veis  
- âœ… Confirme se os padrÃµes HTML nÃ£o mudaram

### **Timeout em sites especÃ­ficos**
```python
# Aumente o timeout para sites lentos
scraper = NewsLinkScraper(request_timeout=60)
```

## ğŸ—ï¸ Fluxo de Dados

```mermaid
graph LR
    A[all-govbr-sites.csv] --> B[main.py scrape]
    B --> C[scraped_urls.csv]
    C --> D[main.py update_urls]
    E[site_urls.yaml] --> D
    D --> F[Updated site_urls.yaml]
```

```
data/input/all-govbr-sites.csv â†’ [scrape] â†’ data/stage/scraped_urls.csv â†’ [update_urls] â†’ data/output/site_urls.yaml
                                          â†‘                                            â†‘
                           data/input/site_urls.yaml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¤ Contribuindo

1. **Fork** o projeto
2. **Crie** uma branch: `git checkout -b feature/nova-funcionalidade`
3. **Implemente** suas mudanÃ§as com testes
4. **Execute** a suÃ­te de testes: `pytest`
5. **Commit** seguindo [Conventional Commits](https://conventionalcommits.org/)
6. **Abra** um Pull Request

### **PadrÃµes de Commit**
```bash
feat: adiciona nova funcionalidade de extraÃ§Ã£o
fix: corrige bug na validaÃ§Ã£o de URLs
docs: atualiza documentaÃ§Ã£o do README
test: adiciona testes para novo mÃ³dulo
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ“ˆ Roadmap

### **v2.0** (PrÃ³ximas Funcionalidades)
- [ ] Interface web com dashboard
- [ ] Agendamento automÃ¡tico de execuÃ§Ãµes
- [ ] IntegraÃ§Ã£o com APIs de notÃ­cias
- [ ] Monitoramento de mudanÃ§as em tempo real
- [ ] Suporte a outros domÃ­nios governamentais

### **v1.1** (Melhorias Atuais)
- [x] âœ… Arquitetura modular
- [x] âœ… Interface CLI profissional  
- [x] âœ… ValidaÃ§Ã£o inteligente de URLs
- [x] âœ… Pipeline automatizado completo

## ğŸ† Reconhecimentos

- **99.4% de taxa de sucesso** na extraÃ§Ã£o de URLs
- **88.6% de precisÃ£o** na validaÃ§Ã£o com fonte da verdade
- **+79 agÃªncias governamentais** descobertas automaticamente
- **Arquitetura production-ready** com testes completos
